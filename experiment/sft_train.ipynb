{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a68c50e6",
   "metadata": {},
   "source": [
    "### If Execute code on Colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca8d4b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Google Drive mount\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00a5e5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub transformers bitsandbytes datasets\n",
    "# !pip install -U transformers accelerate bitsandbytes torch torchvision torchaudio\n",
    "# !pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d7b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# # Load dataset\n",
    "# dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
    "\n",
    "# # Split\n",
    "# train_valid_test = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "# dev_test = train_valid_test[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# # Group into DatasetDict\n",
    "# dataset_splits = DatasetDict({\n",
    "#     \"train\": train_valid_test[\"train\"],\n",
    "#     \"dev\": dev_test[\"train\"],\n",
    "#     \"test\": dev_test[\"test\"]\n",
    "# })\n",
    "\n",
    "# # Save to disk\n",
    "# dataset_splits.save_to_disk(\"data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a592f363",
   "metadata": {},
   "source": [
    "### IA3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14be712a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, TrainingArguments, Trainer, DataCollatorForLanguageModeling\n",
    "from peft import get_peft_model, IA3Config, TaskType\n",
    "from datasets import load_from_disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8702df2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load tokenizer and model\n",
    "# model name\n",
    "model_id = \"Qwen/Qwen3-1.7B\"\n",
    "\n",
    "# IA3\n",
    "peft_config = IA3Config(\n",
    "    target_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    feedforward_modules=[\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"],\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "# model with IA3\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, trust_remote_code=True)\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72745c37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load dataset (from disk)\n",
    "from datasets import DatasetDict\n",
    "\n",
    "dataset = DatasetDict.load_from_disk(\"/content/drive/MyDrive/LLM/few-shot/data\")\n",
    "\n",
    "train_dataset = dataset[\"train\"]\n",
    "dev_dataset = dataset[\"dev\"]\n",
    "test_dataset = dataset[\"test\"]\n",
    "\n",
    "print(train_dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a6eb03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Preprocessing: format prompt and tokenize\n",
    "def format_pubmedqa(example):\n",
    "    prompt = f\"Question: {example['question']}\\nContext: {example['context']}\\nAnswer:\"\n",
    "    target = example[\"long_answer\"]\n",
    "    full_text = prompt + \" \" + target\n",
    "\n",
    "    tokenized = tokenizer(\n",
    "        full_text,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        max_length=512,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    tokenized[\"labels\"] = tokenized[\"input_ids\"].clone()  # 수정된 부분\n",
    "    return {k: v.squeeze() for k, v in tokenized.items()}\n",
    "\n",
    "tokenized_train = train_dataset.map(format_pubmedqa, remove_columns=train_dataset.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9c42d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./results/qwen_pubmedqa_ia3_sft\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=2,\n",
    "    gradient_accumulation_steps=4,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    fp16=True,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# 5. Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    data_collator=DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm=False)\n",
    ")\n",
    "\n",
    "# 6. Start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecb5d77e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Save model and tokenizer\n",
    "model.save_pretrained(\"./results/qwen_pubmedqa_ia3_sft/final\")\n",
    "tokenizer.save_pretrained(\"./results/qwen_pubmedqa_ia3_sft/final\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae70c3d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Save model to huggingface\n",
    "from huggingface_hub import login, create_repo\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# Huggingface Login\n",
    "login(token = os.environ[\"HUGGINGFACE_TOKEN\"])\n",
    "\n",
    "# repository\n",
    "repo_id = \"[your_huggingface_id]/qwen_pubmedqa_ia3_sft\"\n",
    "create_repo(repo_id, private=False)\n",
    "\n",
    "# model & tokenizer load\n",
    "model_path = \"./results/qwen_pubmedqa_ia3_sft/final\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "# Huggingface Upload\n",
    "model.push_to_hub(repo_id)\n",
    "tokenizer.push_to_hub(repo_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dacdf8a",
   "metadata": {},
   "source": [
    "### Model Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dfc2596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Test\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = '[your_huggingface_id]/qwen_pubmedqa_ia3_sft'\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf13d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"Aspirin reduces the risk of heart attack?\"\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "output = model.generate(**inputs, max_length=200, do_sample=True, top_k=50, top_p=0.95)\n",
    "decode = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "\n",
    "print(\"Answer:\")\n",
    "print(decode)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "few-shot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
