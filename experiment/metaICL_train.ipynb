{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c46a43f2",
   "metadata": {},
   "source": [
    "### Execute on colab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94888562",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Google Drive mount\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d062913",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install huggingface_hub transformers bitsandbytes datasets\n",
    "# !pip install -U transformers accelerate bitsandbytes torch torchvision torchaudio\n",
    "# !pip install --upgrade datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75ad9872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from datasets import load_dataset, DatasetDict\n",
    "\n",
    "# # Load dataset\n",
    "# dataset = load_dataset(\"pubmed_qa\", \"pqa_labeled\")\n",
    "\n",
    "# # Split\n",
    "# train_valid_test = dataset[\"train\"].train_test_split(test_size=0.2, seed=42)\n",
    "# dev_test = train_valid_test[\"test\"].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# # Group into DatasetDict\n",
    "# dataset_splits = DatasetDict({\n",
    "#     \"train\": train_valid_test[\"train\"],\n",
    "#     \"dev\": dev_test[\"train\"],\n",
    "#     \"test\": dev_test[\"test\"]\n",
    "# })\n",
    "\n",
    "# # Save to disk\n",
    "# dataset_splits.save_to_disk(\"data\")\n",
    "\n",
    "# # MetaCLI 데이터셋 생성\n",
    "# biosses = load_dataset(\"biosses\")\n",
    "# glue_sst2 = load_dataset(\"glue\", \"sst2\")\n",
    "# ag_news = load_dataset(\"ag_news\")\n",
    "# trec = load_dataset(\"trec\")\n",
    "\n",
    "# metacli = DatasetDict({\n",
    "#     \"biosses\": biosses[\"train\"] if \"train\" in biosses else biosses,\n",
    "#     \"glue_sst2\": glue_sst2[\"train\"],\n",
    "#     \"ag_news\": ag_news[\"train\"],\n",
    "#     \"trec\": trec[\"train\"]\n",
    "# })\n",
    "\n",
    "# # Save to disk\n",
    "# metacli.save_to_disk(\"data/metacli\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1051582b",
   "metadata": {},
   "source": [
    "### MetaICL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab1726b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, Trainer, TrainingArguments\n",
    "from datasets import load_from_disk, load_dataset\n",
    "from peft import get_peft_model, IA3Config, TaskType\n",
    "from tqdm import tqdm\n",
    "from utils import format_metaicl_prompt\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import default_data_collator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ee00c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 설정 ---\n",
    "model_id = \"Qwen/Qwen3-1.7B\"\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "max_seq_length = 1024\n",
    "k_values = [0, 1, 2, 4, 8, 16]  # 실험할 k 값 목록"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851fcb0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 데이터 로드 ---\n",
    "pubmedqa_data = load_from_disk(\"data\")\n",
    "metacli_data = load_from_disk(\"data/metacli\")\n",
    "\n",
    "# 메타 태스크 구성\n",
    "meta_tasks = {\n",
    "    \"PubMedQA\": pubmedqa_data[\"train\"],\n",
    "    \"Biosses\": metacli_data[\"biosses\"],\n",
    "    \"SST2\": metacli_data[\"glue_sst2\"],\n",
    "    \"AGNews\": metacli_data[\"ag_news\"],\n",
    "    \"TREC\": metacli_data[\"trec\"]\n",
    "}\n",
    "\n",
    "target_task_names = [\"PubMedQA\", \"Biosses\", \"SST2\", \"AGNews\", \"TREC\"]  # 메타 학습에 포함할 태스크들"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26672865",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 출력 예시\n",
    "print(pubmedqa_data)             # PubMedQA 데이터셋 정보\n",
    "print(pubmedqa_data[\"train\"])    # train split 정보\n",
    "print(pubmedqa_data[\"train\"][0]) # 첫 번째 샘플 데이터"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2b87ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"PubMedQA Train Size:\", len(pubmedqa_data[\"train\"]))\n",
    "print(\"Biosses Size: \", len(metacli_data[\"biosses\"]))\n",
    "print(\"SST2 Size: \", len(metacli_data[\"glue_sst2\"]))\n",
    "print(\"News Size: \", len(metacli_data[\"ag_news\"]))\n",
    "print(\"Trec Size: \", len(metacli_data[\"trec\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ddb6f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    default_data_collator,\n",
    ")\n",
    "from peft import get_peft_model, IA3Config\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "# device 설정\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 메타 프롬프트 구성 함수\n",
    "def format_metaicl_prompt(task_name, query_example, k_examples, tokenizer, max_length=768):\n",
    "    def example_to_string(example, task_name, include_output=True):\n",
    "        task_map = {\n",
    "            \"MedNLI\": (\"premise\", \"hypothesis\", \"label\"),\n",
    "            \"PubMedQA\": (\"context\", \"question\", \"final_decision\"),\n",
    "        }\n",
    "        if task_name in task_map:\n",
    "            fields = task_map[task_name]\n",
    "            input_str = f\"{fields[0].capitalize()}: {example.get(fields[0], '')}\\n\"\n",
    "            input_str += f\"{fields[1].capitalize()}: {example.get(fields[1], '')}\\nAnswer:\"\n",
    "            output_str = f\" {example.get(fields[2], '')}\"\n",
    "        else:\n",
    "            input_str = f\"Question: {example.get('question', '')}\\nAnswer:\"\n",
    "            output_str = f\" {example.get('answer', '')}\"\n",
    "\n",
    "        return input_str + output_str if include_output else input_str\n",
    "\n",
    "    prompt_parts = [example_to_string(e, task_name) for e in k_examples]\n",
    "    query_prompt = example_to_string(query_example, task_name, include_output=False)\n",
    "    query_full = example_to_string(query_example, task_name, include_output=True)\n",
    "\n",
    "    prompt_parts.append(query_prompt)\n",
    "    prompt_str = \"\\n\".join(prompt_parts)\n",
    "    full_text = prompt_str + query_full[len(query_prompt):]\n",
    "\n",
    "    tokenized_prompt = tokenizer(prompt_str, truncation=True, max_length=max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
    "    tokenized_full = tokenizer(full_text, truncation=True, max_length=max_length, padding=\"max_length\", return_tensors=\"pt\")\n",
    "\n",
    "    input_ids = tokenized_prompt[\"input_ids\"]\n",
    "    attention_mask = tokenized_prompt[\"attention_mask\"]\n",
    "    labels = tokenized_full[\"input_ids\"]\n",
    "\n",
    "    labels_masked = labels.clone()\n",
    "    labels_masked[labels_masked == tokenizer.pad_token_id] = -100\n",
    "    labels_masked[:, :input_ids.shape[1]] = -100\n",
    "\n",
    "    return input_ids.squeeze(0), attention_mask.squeeze(0), labels_masked.squeeze(0)\n",
    "\n",
    "\n",
    "# Dataset 클래스 정의\n",
    "class MetaICLDataset(Dataset):\n",
    "    def __init__(self, data_list):\n",
    "        self.data = data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        item = self.data[idx]\n",
    "        return {\n",
    "            \"input_ids\": item[\"input_ids\"],\n",
    "            \"attention_mask\": item[\"attention_mask\"],\n",
    "            \"labels\": item[\"labels\"],\n",
    "        }\n",
    "\n",
    "\n",
    "# --- 학습 루프 시작 ---\n",
    "for k_shots_meta_train in k_values:\n",
    "    print(f\"\\n=== Starting training for k = {k_shots_meta_train} ===\\n\")\n",
    "\n",
    "    # 캐시 파일 경로\n",
    "    cache_path = f\"./cache/formatted_data_k{k_shots_meta_train}.pt\"\n",
    "    os.makedirs(\"./cache\", exist_ok=True)\n",
    "\n",
    "    if os.path.exists(cache_path):\n",
    "        print(f\"Loading cached data from {cache_path}\")\n",
    "        formatted_data = torch.load(cache_path)\n",
    "    else:\n",
    "        print(\"Generating formatted data...\")\n",
    "        formatted_data = []\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for task_name, dataset in meta_tasks.items():\n",
    "                print(f\"Formatting meta-training data for {task_name} (k={k_shots_meta_train})...\")\n",
    "                dataset_list = list(dataset)\n",
    "                num_samples_per_task = min(len(dataset_list), 1000)\n",
    "\n",
    "                for _ in tqdm(range(num_samples_per_task), desc=f\"Processing {task_name}\"):\n",
    "                    try:\n",
    "                        if len(dataset_list) < k_shots_meta_train + 1:\n",
    "                            continue\n",
    "                        sampled_examples = random.sample(dataset_list, k_shots_meta_train + 1)\n",
    "                        support_examples = sampled_examples[:k_shots_meta_train]\n",
    "                        query_example = sampled_examples[k_shots_meta_train]\n",
    "\n",
    "                        input_ids, attention_mask, labels = format_metaicl_prompt(\n",
    "                            task_name, query_example, support_examples, tokenizer, max_length=768\n",
    "                        )\n",
    "\n",
    "                        formatted_data.append({\n",
    "                            \"input_ids\": input_ids,\n",
    "                            \"attention_mask\": attention_mask,\n",
    "                            \"labels\": labels,\n",
    "                        })\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error processing sample from {task_name}: {e}\")\n",
    "                        continue\n",
    "\n",
    "        torch.save(formatted_data, cache_path)\n",
    "        print(f\"Saved formatted data to {cache_path}\")\n",
    "\n",
    "    # 모델 및 토크나이저 로드\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "    base_model = AutoModelForCausalLM.from_pretrained(model_id).to(device)\n",
    "\n",
    "    # PEFT 설정\n",
    "    peft_config = IA3Config(\n",
    "        target_modules=[\"q_proj\", \"v_proj\"],\n",
    "        feedforward_modules=[\"q_proj\", \"v_proj\"],\n",
    "        task_type=\"CAUSAL_LM\"\n",
    "    )\n",
    "    model = get_peft_model(base_model, peft_config).to(device)\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # 데이터셋 로딩\n",
    "    train_dataset = MetaICLDataset(formatted_data)\n",
    "\n",
    "    # 학습 설정\n",
    "    output_dir = f\"./results/metaicl_qwen_meta/k_{k_shots_meta_train}\"\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=output_dir,\n",
    "        per_device_train_batch_size=2,\n",
    "        gradient_accumulation_steps=2,\n",
    "        num_train_epochs=1,\n",
    "        learning_rate=5e-5,\n",
    "        fp16=True,\n",
    "        save_total_limit=2,\n",
    "        logging_dir=f\"./logs/metaicl/k_{k_shots_meta_train}\",\n",
    "        logging_steps=50,\n",
    "        save_strategy=\"epoch\",\n",
    "        report_to=\"none\",\n",
    "        torch_compile=True,\n",
    "    )\n",
    "\n",
    "    # Trainer 정의\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=train_dataset,\n",
    "        data_collator=default_data_collator,\n",
    "    )\n",
    "\n",
    "    # DataLoader 병렬화 설정\n",
    "    trainer.train_dataloader = lambda: DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=training_args.per_device_train_batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=default_data_collator,\n",
    "        num_workers=4,\n",
    "    )\n",
    "\n",
    "    # 학습 시작\n",
    "    trainer.train()\n",
    "\n",
    "    # 모델 저장\n",
    "    trainer.save_model(f\"{output_dir}/final\")\n",
    "    tokenizer.save_pretrained(f\"{output_dir}/final\")\n",
    "\n",
    "    print(f\"\\n=== Finished training for k = {k_shots_meta_train} ===\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "513d4e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "from peft import PeftModel\n",
    "from huggingface_hub import HfApi, create_repo, upload_folder, login\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()\n",
    "os.environ[\"HUGGINGFACE_TOKEN\"] = os.getenv(\"HUGGINGFACE_TOKEN\")\n",
    "\n",
    "# Huggingface Login\n",
    "login(token = os.environ[\"HUGGINGFACE_TOKEN\"])\n",
    "\n",
    "\n",
    "# 사용자 설정\n",
    "k_values = [0, 1, 2, 4, 8, 16]\n",
    "hf_username = \"yerim00\"\n",
    "\n",
    "for k in k_values:\n",
    "    model_path = f\"./results/metaicl_qwen_meta/k_{k}/final\"\n",
    "    repo_name = f\"metaicl-peft-k{k}\"\n",
    "    repo_id = f\"{hf_username}/{repo_name}\"\n",
    "\n",
    "    create_repo(repo_id, exist_ok=True)\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(model_path, trust_remote_code=True)\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)\n",
    "\n",
    "    model.push_to_hub(repo_id)\n",
    "    tokenizer.push_to_hub(repo_id)\n",
    "\n",
    "    print(f\"Uploaded k={k} model to: https://huggingface.co/{repo_id}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "few-shot",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9.21"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
